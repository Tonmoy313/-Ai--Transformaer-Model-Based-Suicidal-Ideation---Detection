{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPxTFYL/lYVOnsoQ9t/qu1r"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"q-9LJDetrpNp"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","from sklearn.metrics import accuracy_score\n","from tqdm.auto import tqdm\n","\n","# Define your CNN-BiLSTM model\n","class CNNBiLSTM(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_classes, kernel_size, dropout_prob):\n","        super(CNNBiLSTM, self).__init__()\n","\n","        # CNN layer\n","        self.cnn = nn.Conv1d(in_channels=embedding_dim, out_channels=hidden_dim, kernel_size=kernel_size)\n","\n","        # BiLSTM layer\n","        self.bilstm = nn.LSTM(input_size=hidden_dim, hidden_size=hidden_dim, bidirectional=True, batch_first=True)\n","\n","        # Fully connected layer\n","        self.fc = nn.Linear(hidden_dim * 2, num_classes)\n","\n","        # Dropout for regularization\n","        self.dropout = nn.Dropout(dropout_prob)\n","\n","    def forward(self, x):\n","        # Input shape: (batch_size, sequence_length, embedding_dim)\n","        x = x.permute(0, 2, 1)  # Reshape for CNN input: (batch_size, embedding_dim, sequence_length)\n","\n","        # CNN\n","        x = self.cnn(x)\n","\n","        # BiLSTM\n","        x, _ = self.bilstm(x)\n","\n","        # Global max pooling\n","        x = torch.max(x, dim=1).values\n","\n","        # Fully connected layer\n","        x = self.fc(x)\n","\n","        return x\n","\n","# Set hyperparameters\n","vocab_size = 10000  # Adjust based on your dataset\n","embedding_dim = 100\n","hidden_dim = 64\n","num_classes = 2\n","kernel_size = 3\n","dropout_prob = 0.5\n","\n","# Instantiate the model\n","model = CNNBiLSTM(vocab_size, embedding_dim, hidden_dim, num_classes, kernel_size, dropout_prob)\n","\n","# Set device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","\n","# Set up your optimizer and loss function (adjust as needed)\n","optimizer = optim.Adam(model.parameters(), lr=3e-4)\n","criterion = nn.CrossEntropyLoss()\n","\n","# Training loop\n","num_epochs = 5\n","for epoch in range(num_epochs):\n","    model.train()\n","    for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}/{num_epochs}\"):\n","        inputs, labels = batch['input_ids'].to(device), batch['labels'].to(device)\n","\n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","# Evaluation\n","model.eval()\n","all_preds = []\n","all_labels = []\n","\n","with torch.no_grad():\n","    for batch in tqdm(validation_dataloader, desc=\"Validation\"):\n","        inputs, labels = batch['input_ids'].to(device), batch['labels'].to(device)\n","        outputs = model(inputs)\n","        predictions = torch.argmax(outputs, dim=1)\n","\n","        all_preds.extend(predictions.cpu().numpy())\n","        all_labels.extend(labels.cpu().numpy())\n","\n","accuracy = accuracy_score(all_labels, all_preds)\n","print(f\"Validation Accuracy: {accuracy * 100:.2f}%\")\n"]}]}